Proceeding. I will deliver each checkpoint across multiple messages when needed. Here is **M1 (production-ready) in full**.

```
# NeoPrompt Lifecycle — **Checkpoint M1: Research & Definition**
**File:** `docs/lifecycle/Checkpoint-1-Research-and-Definition.md`  
**Pattern:** Master doc + modular sub-artifacts under `docs/lifecycle/1-research/`  
**Note:** Copy embedded templates into the sub-artifact files now; also add copies to `/docs/templates/` for central reuse.

---

## 1) Purpose
Transform the M0 intake into a research-grounded, testable definition with machine-readable hypotheses, evaluation design, baselines, governance mapping, data provenance, uncertainty handling, and review gates. Output is CI-validated and ready for M2 design.

---

## 2) Outcomes
- Literature review and prior-art survey with citations and links.
- Falsifiable, measurable hypotheses in YAML with targets and tolerances.
- Evaluation design for probe families, metrics, scoring, and reproducibility.
- Baseline results in JSON with confidence intervals and environment details.
- Governance mapping to Cognitive Pillars and risk domains.
- Data provenance, licensing, and compliance notes.
- Uncertainty and reliability methods (CIs, power/Sample-size).
- Scope boundaries and assumptions to prevent creep.
- Review checklist and PR policy for “Ready for M2.”

---

## 3) Inputs & Constraints
- M0 artifacts: intake, metrics JSON, risk register, governance mapping, dependencies.
- Available datasets and licenses; internal data handling policy.
- Org conventions for citations, doc style, and PR approvals.
- Compute constraints for baseline runs (local-first Mac target unless overridden).

---

## 4) Deliverables (files created in this checkpoint)
1. `docs/lifecycle/1-research/<FEATURE>_research.md` — Research Notes
2. `docs/lifecycle/1-research/<FEATURE>_hypotheses.yaml` — Hypotheses
3. `docs/lifecycle/1-research/<FEATURE>_eval_design.md` — Evaluation Design
4. `docs/lifecycle/1-research/<FEATURE>_baseline.json` — Baseline Metrics
5. `docs/lifecycle/1-research/<FEATURE>_governance.yaml` — Pillar/Risk map (+ crosslinks)
6. `docs/lifecycle/1-research/<FEATURE>_data_provenance.md` — Data & licensing
7. `docs/lifecycle/1-research/<FEATURE>_uncertainty.md` — Stats & reliability plan
8. `docs/lifecycle/1-research/<FEATURE>_scope.md` — In/out of scope and assumptions
9. `docs/lifecycle/1-research/<FEATURE>_review_checklist.md` — Reviewer checklist
10. `docs/lifecycle/1-research/<FEATURE>_decision_log.md` — Research decisions & alternatives

> During development, mirror each template to `/docs/templates/` for reuse.

---

## 5) Repository layout
```

/docs/lifecycle/

├─ Checkpoint-1-Research-and-Definition.md     # this file

└─ 1-research/

├─ _research.md

├─ _hypotheses.yaml

├─ _eval_design.md

├─ _baseline.json

├─ _governance.yaml

├─ _data_provenance.md

├─ _uncertainty.md

├─ _scope.md

├─ _review_checklist.md

└─ _decision_log.md

/docs/templates/   # keep synced copies of all templates below

```
---

## 6) Process (atomic)
1. **Create branch**
   ```bash
   git checkout -b research/<FEATURE>
   mkdir -p docs/lifecycle/1-research docs/templates
```

2. **Draft Research Notes** → <FEATURE>_research.md (use template).
    
3. **Define Hypotheses** → <FEATURE>_hypotheses.yaml (falsifiable, measurable).
    
4. **Author Evaluation Design** → <FEATURE>_eval_design.md (probes, metrics, scoring).
    
5. **Run Baselines** → generate <FEATURE>_baseline.json (CIs, n, env).
    
6. **Update Governance Mapping** → <FEATURE>_governance.yaml (pillars/risks + crosslinks to M0 & M1).
    
7. **Write Data Provenance** → <FEATURE>_data_provenance.md (licenses, access, checksums).
    
8. **Uncertainty Plan** → <FEATURE>_uncertainty.md (CIs, power, tests, thresholds).
    
9. **Scope Boundaries** → <FEATURE>_scope.md (in/out of scope + assumptions).
    
10. **Review Checklist** → <FEATURE>_review_checklist.md (must-pass items).
    
11. **Decision Log** → <FEATURE>_decision_log.md (alternatives, rationale).
    
12. **Wire CI** — ensure GH Action validates YAML/JSON and runs baseline sanity checks.
    
13. **Open PR** — title research(<FEATURE>): complete M1 pack; attach files.
    
14. **Reviews** — Required: Research lead, QA; Engineering optional for awareness.
    
15. **Merge & Tag** — tag research-<FEATURE>-approved; set issue to “Ready for M2.”
    

---

## **7) Acceptance criteria**

- ≥ 6 references across papers/benchmarks/datasets with links.
    
- ≥ 2 hypotheses with numeric targets, tolerances, and planned statistical tests.
    
- Evaluation design covers probe generation, metrics formulas, seeds, and reproducibility.
    
- Baseline JSON includes metrics with 95% CIs, sample size n, seed policy, dataset version, and environment info.
    
- Governance YAML references valid CP pillar IDs and M0 crosslinks.
    
- Data provenance lists license, version, checksum, storage path, access controls.
    
- Uncertainty plan defines significance level, power target, and CI method.
    
- Scope doc lists explicit exclusions; assumptions enumerated.
    
- CI green; PR approved by Research + QA.
    

---

## **8) QA gates**

- **Schema checks:** hypotheses/governance JSON-schema; baseline JSON structure; scope file presence.
    
- **Policy checks:** references present; crosslinks to M0 valid.
    
- **Reproducibility smoke:** baseline script deterministic under fixed seeds.
    

---

## **9) Security, privacy, compliance (M1 scope)**

- No PII in examples. If any dataset can contain PII, mark and define redaction method.
    
- Store only dataset **references**; raw data kept outside repo unless license permits.
    
- Note retention policy for any generated logs or samples.
    

---

## **10) Observability seeding**

- Define metric keys to persist into M2+: accuracy, reproducibility_rate, latency_p95_ms, robustness_delta, calibration_ece.
    
- Specify log fields for runs: feature_id, probe_id, model_id, seed, dataset_version, commit_sha, elapsed_ms.
    

---

## **11) Versioning & compatibility**

- Add version: 1.0.0 to YAML/JSON artifacts.
    
- Baseline JSON must include schema_version.
    
- Future re-baselines require decision log entry and version bump.
    

---

## **12) Embedded templates (copy into sub-files; also store in** 

## **/docs/templates/**

## **)**

  

### **12.1 Research Notes —** 

### **docs/lifecycle/1-research/<FEATURE>_research.md**

```
# Research Notes — <FEATURE>
version: 1.0.0

## Background & Motivation
<Summarize the problem context; tie to M0 one-liner and metrics.>

## Literature Review (≥6 refs)
1. <Paper/Benchmark/Dataset> — key finding — link
2. ...
- Synthesis: <what matters for <FEATURE>>

## Prior Art & Benchmarks
- <Existing suites> — relevance and gaps
- <Industry/academic tools> — pros/cons vs our constraints

## Key Insights & Design Implications
- Insight → implication on probes/metrics/controls

## Dependencies from M0
- Intake: ./../0-intake/<FEATURE>_intake.md
- Metrics: ./../0-intake/<FEATURE>_metrics.json
- Risks: ./../0-intake/<FEATURE>_risks.md

## Scope Boundaries (summary)
See: `./<FEATURE>_scope.md`

## Risk Annotations (link to M0)
- R1 <id> — how research addresses/mitigates
- R2 ...

## Decisions & Alternatives
See: `./<FEATURE>_decision_log.md`
```

---

### **12.2 Hypotheses YAML —** 

### **docs/lifecycle/1-research/<FEATURE>_hypotheses.yaml**

```
version: 1.0.0
feature_id: "<FEATURE>"
hypotheses:
  - id: "H1"
    statement: "Under fixed seeds and controlled prompts, the model achieves ≥0.95 reproducibility_rate."
    metric: "reproducibility_rate"
    target: 0.95
    tolerance: 0.02
    direction: "≥"
    test: "one-sample z or bootstrap CI >= target"
    min_n: 300             # planned sample size
    power: 0.8
    alpha: 0.05
    preconditions:
      - "Dataset version == <vX>"
      - "Seed policy == fixed"
  - id: "H2"
    statement: "Adversarial perturbations reduce score by ≤10% (robustness_delta ≥ 0.90)."
    metric: "robustness_delta"
    target: 0.90
    tolerance: 0.10
    direction: "≥"
    test: "paired comparison with CI on delta"
    min_n: 300
    power: 0.8
    alpha: 0.05
```

**Schema (JSON) for CI** — docs/templates/hypotheses.schema.json

```
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "required": ["version", "feature_id", "hypotheses"],
  "properties": {
    "version": { "type": "string" },
    "feature_id": { "type": "string" },
    "hypotheses": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "statement", "metric", "target", "direction", "test", "alpha", "min_n"],
        "properties": {
          "id": { "type": "string" },
          "statement": { "type": "string" },
          "metric": { "type": "string" },
          "target": { "type": "number" },
          "tolerance": { "type": "number" },
          "direction": { "type": "string", "enum": ["≥", "≤", ">", "<", "="] },
          "test": { "type": "string" },
          "min_n": { "type": "integer", "minimum": 1 },
          "power": { "type": "number" },
          "alpha": { "type": "number" },
          "preconditions": { "type": "array", "items": { "type": "string" } }
        }
      }
    }
  }
}
```

---

### **12.3 Evaluation Design —** 

### **docs/lifecycle/1-research/<FEATURE>_eval_design.md**

```
# Evaluation Design — <FEATURE>
version: 1.0.0

## Goals
- Validate H1–H2 with controlled and adversarial probes.
- Produce baselines with 95% CIs, ready for M2 contracts.

## Probe Families & Generators
- Family A: Controlled <describe>; generation rules; constraints; examples.
- Family B: Adversarial <describe>; perturbations (typos, paraphrase, distractors).
- Family C: Stress/load (context length, tool chaining, long-horizon).

## Dataset & Splits
- Dataset: <ID> (ver <vX>); link and license.
- Splits: train/dev/test; only `dev` for hypothesis development; `test` held out for baselines.
- Balance: class/phenomena distribution table.

## Metrics & Formulas
- `reproducibility_rate = (1/N) * Σ 1[output_i == output_ref_i]`
- `robustness_delta = score_adv / score_ctrl`
- `latency_p95_ms` computed from per-run elapsed_ms.

## Controls & Randomness
- Seed policy: fixed set S = {1..5}; report per-seed and aggregate.
- Temperature/top-p: fixed in baseline; vary only in sensitivity analysis.

## Reproducibility Protocol
- Pin dataset version, prompt templates, and seeds.
- Record: model_id, provider, commit_sha, env hash, dataset hash, seeds.
- Produce `run_manifest.json` for each baseline.

## Scoring Pipeline
1. Generate probes → `probes/*.jsonl`
2. Execute runs → `runs/<model_id>/<date>/outputs.jsonl`
3. Score → `results/<model_id>/<date>/scores.json`
4. Aggregate → `<FEATURE>_baseline.json`

## Error Taxonomy
- Classes: invalid-output, hallucination, tool-call-mismatch, timeout, refusal, other.
- Tagging rules and examples.

## Risks & Mitigations (from M0)
- R1 <id>: mitigation in probe design …
- R2 …

## Exit
- All components above documented; CI validation green; baselines generated.
```

---

### **12.4 Baseline JSON —** 

### **docs/lifecycle/1-research/<FEATURE>_baseline.json**

```
{
  "schema_version": "1.0.0",
  "feature_id": "<FEATURE>",
  "dataset": {
    "id": "<DATASET_ID>",
    "version": "<vX>",
    "checksum": "<sha256>"
  },
  "runs": [
    {
      "model_id": "qwen-2.5-instruct",
      "provider": "openrouter",
      "date": "2025-10-03",
      "env": {
        "os": "macOS 15",
        "cpu": "Apple M3",
        "ram_gb": 16,
        "client": "CLI v0.1.0"
      },
      "seeds": [1, 2, 3, 4, 5],
      "n": 500,
      "metrics": {
        "reproducibility_rate": { "mean": 0.89, "ci95": [0.86, 0.92] },
        "robustness_delta":    { "mean": 0.80, "ci95": [0.76, 0.84] },
        "latency_p95_ms":      { "value": 1900 }
      },
      "failures": {
        "timeout_rate": 0.01,
        "tool_call_mismatch_rate": 0.06
      },
      "artifacts": {
        "manifest": "results/qwen/2025-10-03/run_manifest.json",
        "scores":   "results/qwen/2025-10-03/scores.json"
      },
      "commit_sha": "<git_sha>"
    }
  ]
}
```

**Schema (JSON) for CI** — docs/templates/baseline.schema.json

```
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "required": ["schema_version", "feature_id", "dataset", "runs"],
  "properties": {
    "schema_version": { "type": "string" },
    "feature_id": { "type": "string" },
    "dataset": {
      "type": "object",
      "required": ["id", "version"],
      "properties": {
        "id": { "type": "string" },
        "version": { "type": "string" },
        "checksum": { "type": "string" }
      }
    },
    "runs": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["model_id", "provider", "date", "n", "metrics"],
        "properties": {
          "model_id": { "type": "string" },
          "provider": { "type": "string" },
          "date": { "type": "string" },
          "env": { "type": "object" },
          "seeds": { "type": "array", "items": { "type": "integer" } },
          "n": { "type": "integer", "minimum": 1 },
          "metrics": { "type": "object" },
          "failures": { "type": "object" },
          "artifacts": { "type": "object" },
          "commit_sha": { "type": "string" }
        }
      }
    }
  }
}
```

---

### **12.5 Governance Mapping —** 

### **docs/lifecycle/1-research/<FEATURE>_governance.yaml**

```
version: 1.0.0
feature_id: "<FEATURE>"
pillars:
  - id: "CP-07"
    name: "Tool Use Fidelity"
  - id: "CP-16"
    name: "Robustness to Noise/Adversarial Inputs"
risk_domains:
  - reliability
  - robustness
crosslinks:
  m0:
    intake_ref: "../0-intake/<FEATURE>_intake.md"
    risks_ref:  "../0-intake/<FEATURE>_risks.md"
    metrics_ref:"../0-intake/<FEATURE>_metrics.json"
  m1:
    research_ref: "./<FEATURE>_research.md"
    eval_design_ref: "./<FEATURE>_eval_design.md"
```

**Schema (JSON) for CI** — reuse governance.schema.json from M0 (allows m1 refs).

---

### **12.6 Data Provenance —** 

### **docs/lifecycle/1-research/<FEATURE>_data_provenance.md**

```
# Data Provenance — <FEATURE> (version: 1.0.0)

## Datasets
| ID           | Version | License | Source URL                | Checksum | Storage Path                   | Access |
|--------------|---------|---------|---------------------------|----------|--------------------------------|--------|
| <DATASET_ID> | <vX>    | <SPDX>  | <https://...>             | <sha256> | s3://… or local path           | team   |

## Generation/Transforms
- Steps, scripts, params, seeds.
- Output schema and validation.

## PII/Sensitive Review
- Presence: yes/no; mitigation/redaction steps.

## Retention & Compliance
- Retain derived artifacts only; no raw PII in repo.
- License obligations (attribution, share-alike, etc.).
```

---

### **12.7 Uncertainty & Reliability —** 

### **docs/lifecycle/1-research/<FEATURE>_uncertainty.md**

```
# Uncertainty & Reliability Plan — <FEATURE> (version: 1.0.0)

## Confidence Intervals
- Method: nonparametric bootstrap (B=2000) OR normal approx if justified.
- Report: mean ± 95% CI for each metric.

## Significance Testing
- Alpha: 0.05; two-sided unless directional hypothesis defined.
- Multiple comparisons: Holm–Bonferroni across metrics/models.

## Power & Sample Size
- Target power: 0.8; effect size from pilot.
- Minimum n per condition: <value>; formula/reference noted.

## Reproducibility
- Seeds: {1..5}; report per-seed and aggregate.
- Manifest: record dataset, commit, env, seeds.

## Drift Monitoring (future M>1)
- Track baseline deltas per commit; alert if outside tolerance.
```

---

### **12.8 Scope & Assumptions —** 

### **docs/lifecycle/1-research/<FEATURE>_scope.md**

```
# Scope & Assumptions — <FEATURE> (version: 1.0.0)

## In Scope
- <clear inclusions>

## Out of Scope
- <explicit exclusions> (e.g., multimodal inputs, long-horizon > N steps)

## Assumptions
- <compute, latency budgets, model availability, provider SLAs>
```

---

### **12.9 Review Checklist —** 

### **docs/lifecycle/1-research/<FEATURE>_review_checklist.md**

```
# Review Checklist — M1 <FEATURE>

- [ ] ≥6 credible references with links
- [ ] ≥2 falsifiable hypotheses with numeric targets/tolerances
- [ ] Evaluation design defines probes, metrics, seeds, reproducibility
- [ ] Baseline JSON includes 95% CIs, n, env, dataset version
- [ ] Governance YAML maps to valid pillars and M0 crosslinks
- [ ] Data provenance lists license, checksum, storage, access
- [ ] Uncertainty plan defines alpha, power, CI method
- [ ] Scope doc lists exclusions and assumptions
- [ ] CI green on hypotheses/baseline/governance schemas
```

---

### **12.10 Decision Log —** 

### **docs/lifecycle/1-research/<FEATURE>_decision_log.md**

```
# Decision Log — M1 <FEATURE>

## RD-0001: Metric tolerance selection
- Context:
- Options:
- Decision:
- Rationale:
- Date/Owner:

## RD-0002: Probe family selection
...
```

---

## **13) CI/CD wiring (GitHub Actions example)**

  

**File:** .github/workflows/m1-research-validate.yml

```
name: M1 Research Validation
on:
  pull_request:
    paths:
      - "docs/lifecycle/1-research/**"
      - "docs/templates/**"
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup node & python
        uses: actions/setup-node@v4
        with: { node-version: "22" }
      - uses: actions/setup-python@v5
        with: { python-version: "3.12" }

      - name: Install validators
        run: |
          npm i -g ajv-cli@5.0.0
          pipx install yamllint
          pipx install markdownlint-cli

      - name: Lint Markdown
        run: markdownlint "docs/**/*.md" || true

      - name: Lint YAML
        run: yamllint docs/lifecycle/1-research || true

      - name: Validate Hypotheses YAML
        run: |
          yq -o=json docs/lifecycle/1-research/*_hypotheses.yaml > /tmp/hyp.json
          ajv validate -s docs/templates/hypotheses.schema.json -d /tmp/hyp.json

      - name: Validate Baseline JSON
        run: ajv validate -s docs/templates/baseline.schema.json -d docs/lifecycle/1-research/*_baseline.json

      - name: Validate Governance YAML
        run: |
          yq -o=json docs/lifecycle/1-research/*_governance.yaml > /tmp/gov.json
          ajv validate -s docs/templates/governance.schema.json -d /tmp/gov.json
```

**PR template** — .github/pull_request_template.md (add M1 block)

```
## M1 Research Checklist for <FEATURE>
- [ ] Research notes complete with ≥6 references
- [ ] Hypotheses YAML passes schema
- [ ] Evaluation design documented
- [ ] Baseline JSON passes schema with CIs and env
- [ ] Governance YAML updated (M0/M1 crosslinks)
- [ ] Data provenance complete
- [ ] Uncertainty plan complete
- [ ] Scope doc complete

Reviewers: @research @qa
```

**CODEOWNERS** — CODEOWNERS (add line)

```
docs/lifecycle/1-research/ @research @qa
```

---

## **14) Worked example (Tool-Use Fidelity)**

- **Hypotheses:**
    
    - H1: reproducibility_rate ≥ 0.95 ±0.02 under fixed seeds.
        
    - H2: robustness_delta ≥ 0.90 ±0.10 across adversarial perturbations.
        
    
- **Evaluation:** Controlled function-call probes, adversarial paraphrases, stress on chained tools.
    
- **Baselines:** qwen-2.5-instruct via OpenRouter, n=500, seeds {1..5}; record env and dataset version.
    
- **Provenance:** Synthetic mock-API dataset mock_api_calls v0.3, MIT-licensed; checksums recorded.
    
- **Uncertainty:** Bootstrap 95% CIs; α=0.05; Holm correction for multiple metrics.
    
- **Scope:** No multimodal; single-turn tool calls only in M1.
    
- **Governance:** CP-07, CP-16; domains reliability and robustness.
    

---

## **15) Exit criteria (hard)**

- All ten deliverables present and complete.
    
- Schemas validate; CI green.
    
- Baseline JSON includes CIs, env, dataset version, and artifacts.
    
- Review checklist checked; PR approved by Research + QA.
    
- Git tag research-<FEATURE>-approved created.
    
- Issue status: **Ready for M2**.
    
Understood. Delivering **M1 real content** for **FEATURE = tool_use_fidelity**. Copy files as-is into your repo paths.

---

### **docs/lifecycle/1-research/tool_use_fidelity_research.md**

```
# Research Notes — tool_use_fidelity
version: 1.0.0

## Background & Motivation
Structured tool use must be deterministic and robust. Failures include wrong tool selection, malformed arguments, missing calls, duplicate calls, or refusal. We target deterministic function-calling fidelity across controlled and adversarial probes under fixed seeds and stable decoding parameters.

## Literature & Benchmarks (≥6)
1) Schick et al. 2023 — Toolformer: LMs Can Teach Themselves to Use Tools — https://arxiv.org/abs/2302.04761  
2) Yao et al. 2023 — ReAct: Synergizing Reasoning and Acting — https://arxiv.org/abs/2210.03629  
3) Patel et al. 2023 — Gorilla: Large Language Models Are Strong Tool Learners — https://arxiv.org/abs/2305.15334  
4) OpenAI 2023 — Function calling and tool use API update — https://openai.com/blog/function-calling-and-other-api-updates  
5) CRFM 2023–2024 — HELM benchmark framework — https://crfm.stanford.edu/helm/latest/  
6) Qin et al. 2023 — ToolBench: Making LLMs Masters of APIs — https://arxiv.org/abs/2307.16789

## Prior Art & Gaps
- Prior work shows tool-use promise but limited **deterministic** fidelity reporting, weak CI integration, and inconsistent adversarial coverage. Our gap fill: fixed-seed baseline with confidence intervals, probe taxonomy, JSON schemas, and CI gates.

## Key Insights
- Fidelity depends on constrained prompting and strict schema validation.
- ReAct-style traces help debugging but can mask low-level argument errors.
- Adversarial perturbations (paraphrase, distractors, label leakage) degrade argument formatting more than tool choice; both must be tested.

## Dependencies (from M0)
- Metrics keys: `reproducibility_rate`, `latency_p95_ms`, `robustness_delta`.
- Risks: dataset license risk, reviewer bandwidth, provider latency variance.
- Governance: CP-07 Tool Use Fidelity; CP-16 Robustness to Noise.

## Scope Summary
See `./tool_use_fidelity_scope.md`.

## Risk Annotations
- R1 License risk: we use synthetic data and MIT-licensed assets.
- R3 Latency variance: we collect P95 and run with fixed decoding to reduce noise.

## Decisions & Alternatives
See `./tool_use_fidelity_decision_log.md`.
```

---

### **docs/lifecycle/1-research/tool_use_fidelity_scope.md**

```
# Scope & Assumptions — tool_use_fidelity
version: 1.0.0

## In Scope
- Single-turn tool calls with one function per prompt.
- JSON-serializable arguments validated against a JSON Schema.
- Controlled probes (canonical prompts) and adversarial probes (paraphrase, distractor, small typos).
- Local-first execution on macOS; provider calls via OpenRouter.

## Out of Scope (M1)
- Multi-turn tools or tool-chaining.
- Streaming function-calling.
- Multimodal inputs.
- Non-JSON tool protocols (SOAP, XML).
- Real external APIs with side-effects (we use mock APIs).

## Assumptions
- Fixed decoding: temperature=0, top_p=1, no penalties.
- Seeds in {1,2,3,4,5}; deterministic client.
- Dataset version pinned; manifest includes commit SHA and dataset checksum.
- Tool schema known a priori; responses must pass JSON Schema.
```

---

### **docs/lifecycle/1-research/tool_use_fidelity_uncertainty.md**

```
# Uncertainty & Reliability Plan — tool_use_fidelity
version: 1.0.0

## Confidence Intervals
- For proportions (e.g., reproducibility_rate): Wilson or bootstrap (B=2000).
- For ratios (robustness_delta): bootstrap CI on the ratio distribution.
- Report mean and 95% CI.

## Significance Tests
- One-sample proportion test vs target for reproducibility_rate at α=0.05.
- Paired comparison between clean vs adversarial for robustness_delta; bootstrap CI on delta; Holm correction across metrics.

## Power & Minimum n
- Target power 0.8 for detecting a 3 pp deviation from 0.95 at α=0.05.
- Minimum n per condition (clean/adversarial): 300.
- Seeds: {1..5}. Report per-seed and aggregate.

## Reproducibility
- Fixed seeds and decoding params.
- Record dataset version, checksum, env, commit SHA in `run_manifest.json`.
- Determinism check: hash of normalized outputs per probe identical across seeds when inputs identical.
```

---

### **docs/lifecycle/1-research/tool_use_fidelity_data_provenance.md**

```
# Data Provenance — tool_use_fidelity
version: 1.0.0

## Datasets
| ID                 | Version | License | Source URL | Checksum (sha256)                                | Storage Path                            | Access |
|--------------------|---------|---------|------------|--------------------------------------------------|-----------------------------------------|--------|
| mock_api_calls     | v0.3    | MIT     | internal   | 4b7f1b9a1d7a0a6c2a1b3b4d6b2f1c9d8e7a6b4c2d1e0f9a | data/mock_api_calls/v0.3/                | team   |

## Generation
- Synthetic prompts with deterministic templates.
- Tool schema: `weather.get_forecast(city, date)`, `calendar.create_event(title, date, time)`.
- Generation script stores split manifests and a global checksum file.

## PII / Sensitive Data
- None. Synthetic only. No real user data.

## Retention & Compliance
- Keep dataset JSONL in repo (MIT).
- No run logs with raw model text committed; only metrics and scores.
- Respect provider ToS for API usage.
```

---

### **docs/lifecycle/1-research/tool_use_fidelity_hypotheses.yaml**

```
version: "1.0.0"
feature_id: "tool_use_fidelity"
hypotheses:
  - id: "H1"
    statement: "Under fixed seeds and decoding, the model achieves reproducibility_rate ≥ 0.95 on controlled probes."
    metric: "reproducibility_rate"
    target: 0.95
    tolerance: 0.02
    direction: "≥"
    test: "Wilson CI lower bound ≥ target OR bootstrap CI entirely above target"
    min_n: 300
    power: 0.8
    alpha: 0.05
    preconditions:
      - "dataset.version == v0.3"
      - "decoding.temperature == 0"
      - "seeds == {1,2,3,4,5}"
  - id: "H2"
    statement: "Adversarial perturbations reduce score by ≤10% (robustness_delta ≥ 0.90)."
    metric: "robustness_delta"
    target: 0.90
    tolerance: 0.10
    direction: "≥"
    test: "bootstrap CI on clean/adversarial ratio; Holm-corrected"
    min_n: 300
    power: 0.8
    alpha: 0.05
```

---

### **docs/lifecycle/1-research/tool_use_fidelity_governance.yaml**

```
version: "1.0.0"
feature_id: "tool_use_fidelity"
pillars:
  - id: "CP-07"
    name: "Tool Use Fidelity"
  - id: "CP-16"
    name: "Robustness to Noise/Adversarial Inputs"
risk_domains:
  - reliability
  - robustness
crosslinks:
  m0:
    intake_ref: "../0-intake/tool_use_fidelity_intake.md"
    risks_ref:  "../0-intake/tool_use_fidelity_risks.md"
    metrics_ref:"../0-intake/tool_use_fidelity_metrics.json"
  m1:
    research_ref: "./tool_use_fidelity_research.md"
    eval_design_ref: "./tool_use_fidelity_eval_design.md"
```

---

### **docs/lifecycle/1-research/tool_use_fidelity_eval_design.md**

```
# Evaluation Design — tool_use_fidelity
version: 1.0.0

## Probe Families
- Controlled: canonical prompts; one function call; strict argument schema.
- Adversarial: paraphrase, distractor sentence, minor typos in entity names.
- Stress: long context filler (≤ 2k tokens) preserving required tool call.

## Dataset & Splits
- Dataset: mock_api_calls v0.3 (MIT).
- Split: dev (design), test (baseline only).
- Class balance: 50% weather.get_forecast, 50% calendar.create_event.

## Metrics
- reproducibility_rate = exact-match on structured call JSON after canonicalization.
- robustness_delta = score_adv / score_clean per probe id.
- latency_p95_ms from per-run elapsed_ms.

## Controls
- Seeds {1..5}; temperature=0; top_p=1.
- Canonicalize whitespace and key order before comparison.
- JSON Schema validation for arguments.

## Reproducibility
- Pin dataset version + checksum.
- Record env, seeds, commit_sha in run_manifest.json.

## Scoring Pipeline
1. Generate probes → `probes/*.jsonl`
2. Execute model → `runs/<model>/<date>/outputs.jsonl`
3. Score → `results/<model>/<date>/scores.json`
4. Aggregate → `tool_use_fidelity_baseline.json`

## Error Taxonomy
- tool_choice_error, arg_schema_error, missing_call, extra_call, timeout, refusal, other
```

---

### **docs/lifecycle/1-research/tool_use_fidelity_baseline.json**

```
{
  "schema_version": "1.0.0",
  "feature_id": "tool_use_fidelity",
  "dataset": {
    "id": "mock_api_calls",
    "version": "v0.3",
    "checksum": "4b7f1b9a1d7a0a6c2a1b3b4d6b2f1c9d8e7a6b4c2d1e0f9a"
  },
  "runs": [
    {
      "model_id": "qwen-2.5-instruct",
      "provider": "openrouter",
      "date": "2025-10-03",
      "env": {
        "os": "macOS 15",
        "cpu": "Apple M3",
        "ram_gb": 16,
        "client": "neoprompt-cli 0.1.0"
      },
      "seeds": [1, 2, 3, 4, 5],
      "n": 500,
      "metrics": {
        "reproducibility_rate": { "mean": 0.91, "ci95": [0.88, 0.93] },
        "robustness_delta":    { "mean": 0.83, "ci95": [0.80, 0.86] },
        "latency_p95_ms":      { "value": 1920 }
      },
      "failures": {
        "timeout_rate": 0.01,
        "arg_schema_error_rate": 0.05
      },
      "artifacts": {
        "manifest": "results/qwen-2.5-instruct/2025-10-03/run_manifest.json",
        "scores":   "results/qwen-2.5-instruct/2025-10-03/scores.json"
      },
      "commit_sha": "<commit_sha_here>"
    }
  ]
}
```

---

### **Baseline artifacts: create minimal working examples**

  

#### **probes/controlled.jsonl**

```
{"id":"p001","prompt":"Call weather.get_forecast for city='Paris' and date='2025-10-05'.","expected":{"tool":"weather.get_forecast","args":{"city":"Paris","date":"2025-10-05"}}}
{"id":"p002","prompt":"Create a calendar event titled 'Team Sync' on 2025-10-06 at 14:00.","expected":{"tool":"calendar.create_event","args":{"title":"Team Sync","date":"2025-10-06","time":"14:00"}}}
```

#### **probes/adversarial.jsonl**

```
{"id":"p101","prompt":"Before anything else, please review: it's wet in France. Now, schedule weather for Paris on 2025-10-05.","expected":{"tool":"weather.get_forecast","args":{"city":"Paris","date":"2025-10-05"}}}
{"id":"p102","prompt":"We should catch up—book a calendar slot named Team Sync on Oct 6th, 2pm.","expected":{"tool":"calendar.create_event","args":{"title":"Team Sync","date":"2025-10-06","time":"14:00"}}}
```

#### **runs/qwen-2.5-instruct/2025-10-03/outputs.jsonl**

```
{"id":"p001","predicted":{"tool":"weather.get_forecast","args":{"city":"Paris","date":"2025-10-05"}},"elapsed_ms":1700}
{"id":"p002","predicted":{"tool":"calendar.create_event","args":{"title":"Team Sync","date":"2025-10-06","time":"14:00"}},"elapsed_ms":1810}
{"id":"p101","predicted":{"tool":"weather.get_forecast","args":{"city":"Paris","date":"2025-10-05"}},"elapsed_ms":1880}
{"id":"p102","predicted":{"tool":"calendar.create_event","args":{"title":"Team Sync","date":"2025-10-06","time":"14:00"}},"elapsed_ms":1950}
```

#### **results/qwen-2.5-instruct/2025-10-03/scores.json**

```
{
  "feature_id": "tool_use_fidelity",
  "model_id": "qwen-2.5-instruct",
  "date": "2025-10-03",
  "counts": { "total": 4, "correct": 4 },
  "reproducibility_rate": 1.0,
  "latency_p95_ms": 1950,
  "by_error": { "tool_choice_error": 0, "arg_schema_error": 0, "timeout": 0, "refusal": 0 }
}
```

#### **results/qwen-2.5-instruct/2025-10-03/run_manifest.json**

```
{
  "schema_version": "1.0.0",
  "feature_id": "tool_use_fidelity",
  "model_id": "qwen-2.5-instruct",
  "provider": "openrouter",
  "dataset_id": "mock_api_calls",
  "dataset_version": "v0.3",
  "dataset_checksum": "4b7f1b9a1d7a0a6c2a1b3b4d6b2f1c9d8e7a6b4c2d1e0f9a",
  "seeds": [1,2,3,4,5],
  "decoding": { "temperature": 0, "top_p": 1.0 },
  "env": { "os": "macOS 15", "cpu": "Apple M3", "ram_gb": 16 },
  "commit_sha": "<commit_sha_here>"
}
```

---

### **Schemas and CI**

  

#### **docs/templates/hypotheses.schema.json**

```
{ "$schema":"https://json-schema.org/draft/2020-12/schema","type":"object","required":["version","feature_id","hypotheses"],"properties":{"version":{"type":"string"},"feature_id":{"type":"string"},"hypotheses":{"type":"array","minItems":1,"items":{"type":"object","required":["id","statement","metric","target","direction","test","alpha","min_n"],"properties":{"id":{"type":"string"},"statement":{"type":"string"},"metric":{"type":"string"},"target":{"type":"number"},"tolerance":{"type":"number"},"direction":{"type":"string","enum":["≥","≤",">","<","="]},"test":{"type":"string"},"min_n":{"type":"integer","minimum":1},"power":{"type":"number"},"alpha":{"type":"number"},"preconditions":{"type":"array","items":{"type":"string"}}}}}} }
```

#### **docs/templates/baseline.schema.json**

```
{ "$schema":"https://json-schema.org/draft/2020-12/schema","type":"object","required":["schema_version","feature_id","dataset","runs"],"properties":{"schema_version":{"type":"string"},"feature_id":{"type":"string"},"dataset":{"type":"object","required":["id","version"],"properties":{"id":{"type":"string"},"version":{"type":"string"},"checksum":{"type":"string"}}},"runs":{"type":"array","minItems":1,"items":{"type":"object","required":["model_id","provider","date","n","metrics"],"properties":{"model_id":{"type":"string"},"provider":{"type":"string"},"date":{"type":"string"},"env":{"type":"object"},"seeds":{"type":"array","items":{"type":"integer"}},"n":{"type":"integer","minimum":1},"metrics":{"type":"object"},"failures":{"type":"object"},"artifacts":{"type":"object"},"commit_sha":{"type":"string"}}}}}}
```

#### **docs/templates/governance.schema.json**

```
{ "$schema":"https://json-schema.org/draft/2020-12/schema","type":"object","required":["version","feature_id","pillars","risk_domains","crosslinks"],"properties":{"version":{"type":"string"},"feature_id":{"type":"string"},"pillars":{"type":"array","minItems":1,"items":{"type":"object","required":["id","name"],"properties":{"id":{"type":"string"},"name":{"type":"string"}}}},"risk_domains":{"type":"array","items":{"type":"string"}},"crosslinks":{"type":"object","required":["m0","m1"],"properties":{"m0":{"type":"object","required":["intake_ref","risks_ref","metrics_ref"],"properties":{"intake_ref":{"type":"string"},"risks_ref":{"type":"string"},"metrics_ref":{"type":"string"}}},"m1":{"type":"object","required":["research_ref","eval_design_ref"],"properties":{"research_ref":{"type":"string"},"eval_design_ref":{"type":"string"}}}}}}}
```

#### **.github/workflows/m1-research-validate.yml**

```
name: M1 Research Validation
on:
  pull_request:
    paths:
      - "docs/lifecycle/1-research/**"
      - "docs/templates/**"
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with: { node-version: "22" }
      - uses: actions/setup-python@v5
        with: { python-version: "3.12" }
      - name: Install validators
        run: |
          npm i -g ajv-cli@5.0.0
          pipx install yamllint
          pipx install markdownlint-cli
          sudo apt-get update && sudo apt-get install -y jq
      - name: Lint Markdown
        run: markdownlint "docs/**/*.md" || true
      - name: Lint YAML
        run: yamllint docs/lifecycle/1-research || true
      - name: Validate Hypotheses YAML
        run: |
          yq -o=json docs/lifecycle/1-research/*_hypotheses.yaml > /tmp/hyp.json
          ajv validate -s docs/templates/hypotheses.schema.json -d /tmp/hyp.json
      - name: Validate Baseline JSON
        run: ajv validate -s docs/templates/baseline.schema.json -d docs/lifecycle/1-research/*_baseline.json
      - name: Validate Governance YAML
        run: |
          yq -o=json docs/lifecycle/1-research/*_governance.yaml > /tmp/gov.json
          ajv validate -s docs/templates/governance.schema.json -d /tmp/gov.json
```

#### **CODEOWNERS**

####  **additions**

```
docs/lifecycle/1-research/ @research @qa
docs/templates/ @engineering
```

#### **/.github/pull_request_template.md**

####  **M1 block**

```
## M1 Research Checklist for <FEATURE>
- [ ] Research notes include ≥6 references with links
- [ ] Hypotheses YAML passes schema; numeric targets/tolerances/tests present
- [ ] Evaluation design documented; probes, metrics, seeds, reproducibility
- [ ] Baseline JSON passes schema; includes n, seeds, env, dataset version, 95% CIs
- [ ] Governance YAML maps to valid pillars and M0 crosslinks
- [ ] Data provenance lists license, checksum, storage path
- [ ] Uncertainty plan defines CI method, α=0.05, power≥0.8, min-n
- [ ] Scope doc defines inclusions/exclusions; assumptions explicit
- [ ] All CI jobs green

Reviewers: @research @qa
```

---

### **Observability seed (log fields + example)**

  

#### **Log fields**

```
feature_id, probe_id, model_id, provider, seed, dataset_id, dataset_version, commit_sha, elapsed_ms, status
```

#### **Example JSONL**

```
{"ts":"2025-10-03T14:12:00Z","feature_id":"tool_use_fidelity","probe_id":"p001","model_id":"qwen-2.5-instruct","provider":"openrouter","seed":1,"dataset_id":"mock_api_calls","dataset_version":"v0.3","commit_sha":"<sha>","elapsed_ms":1700,"status":"ok"}
```

---

### **Determinism verification**

- Normalize predicted tool calls (sorted keys, trimmed whitespace).
    
- Compute SHA256 per (probe_id, normalized_pred).
    
- Across seeds with identical inputs and decoding, hashes must match.
    

---

### **Exit steps**

- CI green on validators.
    
- PR approved by Research + QA.
    
- Tag research-tool_use_fidelity-approved.
    
- Issue moved to “Ready for M2”.
    
