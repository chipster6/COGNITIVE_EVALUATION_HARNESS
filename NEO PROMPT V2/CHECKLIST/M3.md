Proceeding with **M3**. Production-ready, modular, with concrete content for tool_use_fidelity.

```
# NeoPrompt Lifecycle — **Checkpoint M3: Implementation Planning & Dev Setup**
**File:** `docs/lifecycle/Checkpoint-3-Implementation-Planning-and-Dev-Setup.md`  
**Pattern:** Master doc + sub-artifacts under `docs/lifecycle/3-implementation/` and repo scaffolding in `backend/`, `.github/`, etc.  
**Scope target:** FEATURE = `tool_use_fidelity`

---

## 1) Purpose
Translate M2 design into an executable implementation plan and ready-to-code environment. Produce scaffolds, standards, scripts, and CI that ensure deterministic builds, testing, and local-first runs on macOS.

---

## 2) Outcomes
- Breakdown of work into epics/tasks with owners and estimates.
- Local dev environment reproducible on macOS (and CI).
- Project scaffolding: API skeleton, connector, runner, scorer, schemas, logging, metrics, traces.
- Pre-commit, linting, typing, unit/contract test harness.
- Makefile and task runners.
- CI pipelines for test, lint, type-check, contract tests.
- Deterministic seed policy enforced.

---

## 3) Inputs & Constraints
- M0/M1/M2 artifacts, OpenAPI, JSON Schemas, contracts.
- Local-first Apple Silicon; Python 3.12; Node 22 for tooling where needed.
- Provider: OpenRouter; secrets via env; no secrets in VCS.

---

## 4) Deliverables (files created in this checkpoint)
1. `docs/lifecycle/3-implementation/<FEATURE>_workplan.md` — Epics/tasks plan  
2. `docs/lifecycle/3-implementation/<FEATURE>_dev_env.md` — Dev setup guide  
3. `docs/lifecycle/3-implementation/<FEATURE>_coding_standards.md` — Style & quality gates  
4. `docs/lifecycle/3-implementation/<FEATURE>_test_strategy.md` — Unit/integration/contract tests  
5. `docs/lifecycle/3-implementation/<FEATURE>_review_checklist.md` — M3 checklist  
6. Repo scaffolding (see §7) including:
   - `backend/app/main.py`, `backend/app/routes/tool_use.py`
   - `backend/app/connectors/openrouter.py`
   - `backend/app/eval/runner.py`, `backend/app/eval/scorer.py`, `backend/app/eval/normalize.py`
   - `backend/app/schemas/` (validate responses per M2)
   - `backend/app/obs/metrics.py`, `backend/app/obs/logging_config.py`, `backend/app/obs/tracing.py`
   - `tests/` for unit and contract tests
   - `.pre-commit-config.yaml`, `pyproject.toml`, `Makefile`, `.env.example`
   - CI workflows under `.github/workflows/`

---

## 5) Acceptance criteria
- Local dev up from clean clone in ≤10 minutes on macOS (single Make target).
- Pre-commit enforces black/ruff/mypy/markdownlint/yamllint/jsonlint.
- Unit tests, contract tests, and schema validations pass locally and in CI.
- API boots and serves `/api/tool-use/evaluate` with deterministic behavior.
- Logs, metrics, and minimal tracing enabled.
- Determinism: identical outputs for identical inputs and seeds across two runs.

---

## 6) QA gates
- Lint/type/test pass locally and CI.
- Contract tests assert OpenAPI/JSON Schema compliance.
- Golden-file determinism tests green.
- Security baseline: no secrets in code; env-only; rate limit middleware on.

---

## 7) Repo scaffolding (create now)
```

backend/

app/

**init**.py

main.py

routes/

**init**.py

tool_use.py

connectors/

**init**.py

openrouter.py

eval/

**init**.py

runner.py

scorer.py

normalize.py

schemas/

**init**.py

request_schema.json

response_schema.json

obs/

**init**.py

logging_config.py

metrics.py

tracing.py

tests/

**init**.py

test_tool_use_unit.py

test_tool_use_contract.py

docs/

lifecycle/

3-implementation/

tool_use_fidelity_workplan.md

tool_use_fidelity_dev_env.md

tool_use_fidelity_coding_standards.md

tool_use_fidelity_test_strategy.md

tool_use_fidelity_review_checklist.md

.github/

workflows/

ci-m3.yml

.pre-commit-config.yaml

pyproject.toml

Makefile

.env.example

````
---

## 8) Embedded artifacts

### 8.1 Workplan — `docs/lifecycle/3-implementation/tool_use_fidelity_workplan.md`
```markdown
# Workplan — tool_use_fidelity
version: 1.0.0

## Epics
- E1 API Skeleton & Routing
- E2 OpenRouter Connector
- E3 Runner & Determinism
- E4 Scorer & Error Taxonomy
- E5 Schemas & Validation
- E6 Observability (metrics, logs, traces)
- E7 Tests (unit, contract, golden determinism)
- E8 CI & Pre-commit

## Tasks
- T1 Create API route `/api/tool-use/evaluate` (E1) — 1d — Eng
- T2 Map request/response schemas from M2 (E5) — 0.5d — Eng
- T3 Implement OpenRouter client with timeouts & retries (E2) — 1d — Eng
- T4 Normalization of tool-call JSON and schema validation (E3/E5) — 1d — Eng
- T5 Scorer with metrics aggregation, error taxonomy (E4) — 1d — Eng
- T6 Prometheus metrics and JSON logs (E6) — 0.5d — Eng
- T7 OTel traces minimal spans (E6) — 0.5d — Eng
- T8 Unit tests for runner/scorer/normalizer (E7) — 1d — Eng
- T9 Contract tests vs OpenAPI & schemas (E7) — 1d — QA
- T10 Pre-commit, ruff, mypy, markdownlint, yamllint (E8) — 0.5d — Eng
- T11 CI workflow and cache (E8) — 0.5d — Eng

## Risks
- Provider instability → retry/backoff, circuit breaker
- Non-determinism → enforce seeds, temp=0, top_p=1; normalize outputs
- Schema drift → lock `$id` and compile in CI
````

---

### **8.2 Dev setup —**

### **docs/lifecycle/3-implementation/tool_use_fidelity_dev_env.md**

````
# Dev Environment — tool_use_fidelity
version: 1.0.0

## Requirements
- macOS (Apple Silicon), Python 3.12, Node 22
- Homebrew: `brew install python@3.12 node jq`
- Optional: uv or rye for Python envs

## Setup
```bash
cp .env.example .env
python3.12 -m venv .venv && source .venv/bin/activate
pip install -U pip
pip install -e ".[dev]"
pre-commit install
make run
````

## **Environment Variables (.env)**

```
OPENROUTER_API_KEY=your_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
LOG_LEVEL=INFO
METRICS_PORT=9000
```

## **Run**

```
make run          # uvicorn backend.app.main:app --reload --host 0.0.0.0 --port 8000
make test         # unit + contract
make fmt          # format & lint
make type         # mypy
```

````
---

### 8.3 Coding standards — `docs/lifecycle/3-implementation/tool_use_fidelity_coding_standards.md`
```markdown
# Coding Standards — tool_use_fidelity
version: 1.0.0

- Language: Python 3.12; type hints mandatory; mypy strict
- Style: black + ruff
- Errors: never swallow; map to typed domain errors; provide error codes
- Determinism: set PYTHONHASHSEED, seeds; temp=0, top_p=1
- Logging: JSON-structured; no secrets
- Tests: pytest; fast unit tests (<1s each) + contract tests (<10s)
- Commits: Conventional Commits; small, reviewable
````

---

### **8.4 Test strategy —**

### **docs/lifecycle/3-implementation/tool_use_fidelity_test_strategy.md**

```
# Test Strategy — tool_use_fidelity
version: 1.0.0

## Unit
- normalize(): key order, whitespace; schema validate args
- scorer(): exact-match, error taxonomy; latency aggregation
- connector(): timeout, retry, rate-limit paths (mocked)

## Contract
- Request/response against JSON Schemas
- Acceptance cases A1–A4 from M2

## Golden determinism
- Same inputs & seeds produce identical normalized outputs (hash compare)

## Coverage
- Target ≥90% for eval package
```

---

### **8.5 Review checklist —**

### **docs/lifecycle/3-implementation/tool_use_fidelity_review_checklist.md**

```
# Review Checklist — M3 tool_use_fidelity
- [ ] API boots and serves /api/tool-use/evaluate
- [ ] Determinism proven via golden tests
- [ ] Unit + contract tests green locally
- [ ] Pre-commit hooks installed and run clean
- [ ] CI pipeline green on PR
- [ ] Logs/metrics/traces observable locally
- [ ] No secrets in code; .env.example present
```

---

## **9) Build tooling**

### **9.1**

### **pyproject.toml**

###  **(root)**

```
[project]
name = "neoprompt-backend"
version = "0.1.0"
requires-python = ">=3.12"
dependencies = [
  "fastapi==0.115.0",
  "uvicorn[standard]==0.30.6",
  "httpx==0.27.2",
  "pydantic==2.9.2",
  "prometheus-client==0.20.0",
  "orjson==3.10.7",
  "python-json-logger==2.0.7",
  "opentelemetry-sdk==1.26.0",
  "opentelemetry-instrumentation-fastapi==0.47b0",
  "jsonschema==4.23.0"
]
[project.optional-dependencies]
dev = [
  "pytest==8.3.3",
  "pytest-cov==5.0.0",
  "mypy==1.11.2",
  "ruff==0.6.9",
  "black==24.8.0",
  "types-requests",
  "requests-mock",
  "jsonschema[format,nongpl]==4.23.0",
  "pre-commit==4.0.1"
]

[tool.black]
line-length = 100

[tool.ruff]
line-length = 100
target-version = "py312"
select = ["E","F","I","B","UP","C90"]

[tool.mypy]
python_version = "3.12"
strict = true
warn_unused_ignores = true
warn_redundant_casts = true
disallow_untyped_defs = true

[tool.pytest.ini_options]
testpaths = ["backend/tests"]
addopts = "-q"
```

### **9.2**

### **.pre-commit-config.yaml**

```
repos:
  - repo: https://github.com/psf/black
    rev: 24.8.0
    hooks: [{ id: black }]
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.6.9
    hooks: [{ id: ruff, args: ["--fix"] }]
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.11.2
    hooks: [{ id: mypy }]
  - repo: https://github.com/igorshubovych/markdownlint-cli
    rev: v0.39.0
    hooks: [{ id: markdownlint }]
  - repo: https://github.com/adrienverge/yamllint
    rev: v1.35.1
    hooks: [{ id: yamllint }]
```

### **9.3**

### **Makefile**

```
.PHONY: run test fmt type ci seed

PY ?= .venv/bin/python
UVICORN ?= .venv/bin/uvicorn

export PYTHONHASHSEED=0
export NP_SEED=1

run:
 $(UVICORN) backend.app.main:app --reload --host 0.0.0.0 --port 8000

test:
 $(PY) -m pytest -q

fmt:
 pre-commit run --all-files

type:
 mypy backend

ci: fmt type test
```

### **9.4**

### **.env.example**

```
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
LOG_LEVEL=INFO
METRICS_PORT=9000
```

---

## **10) Minimal implementation stubs**

### **10.1**

### **backend/app/main.py**

```
from fastapi import FastAPI
from backend.app.routes.tool_use import router as tool_use_router
from backend.app.obs.logging_config import configure_logging
from backend.app.obs.metrics import metrics_router
from backend.app.obs.tracing import setup_tracing

def create_app() -> FastAPI:
    configure_logging()
    setup_tracing()
    app = FastAPI(title="NeoPrompt Backend", version="0.1.0")
    app.include_router(tool_use_router, prefix="/api")
    app.include_router(metrics_router)
    return app

app = create_app()
```

### **10.2**

### **backend/app/routes/tool_use.py**

```
from fastapi import APIRouter, HTTPException, Header
from pydantic import BaseModel
from backend.app.eval.runner import run_batch
from backend.app.schemas.request_schema import REQUEST_SCHEMA
from backend.app.schemas.response_schema import RESPONSE_SCHEMA
from jsonschema import validate as js_validate, ValidationError

router = APIRouter()

class EvaluateRequest(BaseModel):
    feature_id: str
    probes: list
    config: dict

@router.post("/tool-use/evaluate")
def evaluate(req: EvaluateRequest, x_api_key: str = Header(default="")):
    if not x_api_key:
        raise HTTPException(status_code=401, detail="Missing API key")
    try:
        js_validate(req.model_dump(), REQUEST_SCHEMA)
    except ValidationError as e:
        raise HTTPException(status_code=400, detail={"error": "schema", "msg": str(e)}) from e
    resp = run_batch(req.model_dump())
    try:
        js_validate(resp, RESPONSE_SCHEMA)
    except ValidationError as e:
        raise HTTPException(status_code=500, detail={"error": "contract", "msg": str(e)}) from e
    return resp
```

### **10.3**

### **backend/app/schemas/request_schema.py**

```
import json, importlib.resources as r
REQUEST_SCHEMA = json.loads(r.files("backend.app.schemas").joinpath("request_schema.json").read_text())
```

### **10.4**

### **backend/app/schemas/response_schema.py**

```
import json, importlib.resources as r
RESPONSE_SCHEMA = json.loads(r.files("backend.app.schemas").joinpath("response_schema.json").read_text())
```

### **10.5 Copy M2 JSON Schemas**

- Put the finalized M2 request.schema.json and response.schema.json into backend/app/schemas/.

### **10.6**

### **backend/app/eval/normalize.py**

```
from typing import Any, Dict
import json

def normalize_tool_call(call: Dict[str, Any]) -> str:
    """Return canonical JSON string for tool call with sorted keys and trimmed strings."""
    def _trim(v):
        if isinstance(v, str): return v.strip()
        if isinstance(v, dict): return {k: _trim(vv) for k,vv in v.items()}
        if isinstance(v, list): return [_trim(v) for v in v]
        return v
    norm = _trim(call)
    return json.dumps(norm, sort_keys=True, separators=(",", ":"))
```

### **10.7**

### **backend/app/eval/scorer.py**

```
from typing import Dict, Any, List
from statistics import quantiles
from backend.app.eval.normalize import normalize_tool_call

class ScoreResult(Dict[str, Any]): pass

def score_batch(records: List[Dict[str, Any]]) -> ScoreResult:
    correct = 0
    elapsed = []
    for r in records:
        expected = normalize_tool_call(r["expected"])
        predicted = normalize_tool_call(r["predicted"])
        ok = (expected == predicted)
        correct += 1 if ok else 0
        elapsed.append(r.get("elapsed_ms", 0))
    total = len(records)
    rate = correct / total if total else 0.0
    p95 = int(quantiles(elapsed, n=20)[18]) if elapsed else 0  # approx p95
    return {
        "counts": {"total": total, "correct": correct},
        "reproducibility_rate": rate,
        "latency_p95_ms": p95
    }
```

### **10.8**

### **backend/app/connectors/openrouter.py**

```
from typing import Dict, Any
import httpx, os

BASE_URL = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
API_KEY = os.getenv("OPENROUTER_API_KEY", "")

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "HTTP-Referer": "neoprompt.local",
    "X-Title": "NeoPrompt"
}

TIMEOUT = httpx.Timeout(15.0, connect=5.0)

async def call_model(prompt: str, model_id: str) -> Dict[str, Any]:
    """Call OpenRouter chat completions and parse tool call JSON from text."""
    if not API_KEY:
        raise RuntimeError("OPENROUTER_API_KEY missing")
    body = {
        "model": model_id,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.0,
        "top_p": 1.0
    }
    async with httpx.AsyncClient(timeout=TIMEOUT) as client:
        r = await client.post(f"{BASE_URL}/chat/completions", headers=HEADERS, json=body)
        r.raise_for_status()
        data = r.json()
        # Expect model to return JSON tool call in content; parsing left simple
        content = data["choices"][0]["message"]["content"]
        # You will replace this with robust JSON extraction
        return {"raw": content}
```

### **10.9**

### **backend/app/eval/runner.py**

```
from typing import Dict, Any, List
import asyncio, json, hashlib, os, time
from backend.app.eval.scorer import score_batch
from backend.app.eval.normalize import normalize_tool_call

def _hash(s: str) -> str:
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

async def _eval_probe(probe: Dict[str, Any], model_id: str) -> Dict[str, Any]:
    # Placeholder: mock execution for M3; replace with real connector in M4
    t0 = time.perf_counter()
    predicted = probe["expected"]  # mock: echo expected
    elapsed_ms = int((time.perf_counter() - t0) * 1000)
    return {"id": probe["id"], "predicted": predicted, "expected": probe["expected"], "elapsed_ms": elapsed_ms, "status": "ok"}

def run_batch(req: Dict[str, Any]) -> Dict[str, Any]:
    os.environ["PYTHONHASHSEED"] = "0"
    feature_id = req["feature_id"]
    probes: List[Dict[str, Any]] = req["probes"]
    cfg = req["config"]
    model_id = cfg["model_id"]

    # Deterministic order
    probes_sorted = sorted(probes, key=lambda p: p["id"])
    results = asyncio.run(asyncio.gather(*[_eval_probe(p, model_id) for p in probes_sorted]))
    scored = score_batch(results)

    # Determinism proof hashes
    hashes = { r["id"]: _hash(normalize_tool_call(r["predicted"])) for r in results }

    return {
        "feature_id": feature_id,
        "model_id": model_id,
        "metrics": scored,
        "by_probe": [{"id": r["id"], "status": r["status"], "elapsed_ms": r["elapsed_ms"]} for r in results],
        "hashes": hashes
    }
```

### **10.10 Observability**

backend/app/obs/logging_config.py

```
import logging, os
from pythonjsonlogger import jsonlogger

def configure_logging():
    level = os.getenv("LOG_LEVEL", "INFO")
    logger = logging.getLogger()
    logger.setLevel(level)
    handler = logging.StreamHandler()
    fmt = jsonlogger.JsonFormatter("%(asctime)s %(levelname)s %(message)s")
    handler.setFormatter(fmt)
    logger.handlers = [handler]
```

backend/app/obs/metrics.py

```
from fastapi import APIRouter, Response
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST, Counter, Histogram

metrics_router = APIRouter()

EVAL_REQS = Counter("eval_requests_total", "Total evaluation requests", ["feature","model"])
EVAL_DUR = Histogram("eval_request_duration_ms", "Eval duration ms", ["feature"], buckets=[100,250,500,1000,1500,2000,3000,5000])

@metrics_router.get("/metrics")
def metrics():
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)
```

backend/app/obs/tracing.py

```
def setup_tracing():
    # Stub; wire OTel exporter in M4
    return
```

---

## **11) Tests**

### **11.1**

### **backend/tests/test_tool_use_unit.py**

```
from backend.app.eval.normalize import normalize_tool_call
from backend.app.eval.scorer import score_batch

def test_normalize():
    a = {"tool":"x","args":{"b":"y","a":"z"}}
    b = {"tool":"x","args":{"a":"z","b":"y"}}
    assert normalize_tool_call(a) == normalize_tool_call(b)

def test_score_batch():
    recs = [
        {"expected":{"tool":"t","args":{"x":"y"}}, "predicted":{"tool":"t","args":{"x":"y"}}, "elapsed_ms":100}
    ]
    s = score_batch(recs)
    assert s["reproducibility_rate"] == 1.0
    assert s["latency_p95_ms"] >= 0
```

### **11.2**

### **backend/tests/test_tool_use_contract.py**

```
import json, importlib.resources as r
from jsonschema import validate

REQ = json.loads(r.files("backend.app.schemas").joinpath("request_schema.json").read_text())
RESP = json.loads(r.files("backend.app.schemas").joinpath("response_schema.json").read_text())

def test_request_schema_example():
    example = {
        "feature_id":"tool_use_fidelity",
        "probes":[
            {"id":"p001","prompt":"Call weather for Paris","expected":{"tool":"weather.get_forecast","args":{"city":"Paris","date":"2025-10-05"}}}
        ],
        "config":{"model_id":"qwen-2.5-instruct","provider":"openrouter","seeds":[1],"decoding":{"temperature":0,"top_p":1}}
    }
    validate(example, REQ)

def test_response_schema_minimal():
    resp = {
        "feature_id":"tool_use_fidelity",
        "model_id":"qwen-2.5-instruct",
        "metrics":{"reproducibility_rate":1.0,"latency_p95_ms":100},
        "by_probe":[{"id":"p001","status":"ok","elapsed_ms":100}]
    }
    validate(resp, RESP)
```

---

## **12) CI**

### **12.1**

### **.github/workflows/ci-m3.yml**

```
name: CI-M3
on:
  pull_request:
    paths:
      - "backend/**"
      - ".pre-commit-config.yaml"
      - "pyproject.toml"
      - "docs/lifecycle/3-implementation/**"
jobs:
  build-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.12" }
      - name: Install
        run: |
          python -m venv .venv
          . .venv/bin/activate
          pip install -U pip
          pip install -e ".[dev]"
      - name: Pre-commit
        run: . .venv/bin/activate && pre-commit run --all-files
      - name: Tests
        run: . .venv/bin/activate && pytest -q
```

---

## **13) Exit criteria (hard)**

- Workplan, dev_env, coding_standards, test_strategy, review_checklist files exist.

- API boots locally. /api/tool-use/evaluate returns schema-valid response with mock runner.

- Unit + contract tests pass locally and in CI.

- Pre-commit hooks enforce style/type.

- Determinism test proves identical hashes on two runs.

- PR approved by Engineering + QA; tag implplan-tool_use_fidelity-approved; issue → **Ready for M4**.

---

## **14) Notes for M4**

- Swap mock _eval_probe with real openrouter.call_model and JSON extraction.

- Add robust JSON-in-text extraction and failure taxonomy mapping.

- Wire OTel exporter and richer metrics.

---

### **backend/app/schemas/request_schema.json**

```
{
  "$id": "https://neoprompt.local/schemas/tool_use_fidelity/request.schema.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "tool_use_fidelity Request",
  "type": "object",
  "required": ["feature_id", "probes", "config"],
  "properties": {
    "feature_id": { "type": "string", "const": "tool_use_fidelity" },
    "probes": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["id", "prompt", "expected"],
        "properties": {
          "id": { "type": "string", "minLength": 1 },
          "prompt": { "type": "string", "minLength": 1 },
          "expected": {
            "type": "object",
            "required": ["tool", "args"],
            "properties": {
              "tool": {
                "type": "string",
                "enum": ["weather.get_forecast", "calendar.create_event"]
              },
              "args": {
                "oneOf": [
                  {
                    "type": "object",
                    "required": ["city", "date"],
                    "properties": {
                      "city": { "type": "string", "minLength": 1 },
                      "date": { "type": "string", "pattern": "^\\d{4}-\\d{2}-\\d{2}$" }
                    },
                    "additionalProperties": false
                  },
                  {
                    "type": "object",
                    "required": ["title", "date", "time"],
                    "properties": {
                      "title": { "type": "string", "minLength": 1 },
                      "date": { "type": "string", "pattern": "^\\d{4}-\\d{2}-\\d{2}$" },
                      "time": { "type": "string", "pattern": "^\\d{2}:\\d{2}$" }
                    },
                    "additionalProperties": false
                  }
                ]
              }
            },
            "additionalProperties": false
          }
        },
        "additionalProperties": false
      }
    },
    "config": {
      "type": "object",
      "required": ["model_id", "provider", "seeds", "decoding"],
      "properties": {
        "model_id": { "type": "string", "minLength": 1 },
        "provider": { "type": "string", "const": "openrouter" },
        "seeds": {
          "type": "array",
          "minItems": 1,
          "items": { "type": "integer" }
        },
        "decoding": {
          "type": "object",
          "required": ["temperature", "top_p"],
          "properties": {
            "temperature": { "type": "number", "const": 0 },
            "top_p": { "type": "number", "const": 1 }
          },
          "additionalProperties": false
        }
      },
      "additionalProperties": false
    }
  },
  "additionalProperties": false
}
```

---

### **backend/app/schemas/response_schema.json**

```
{
  "$id": "https://neoprompt.local/schemas/tool_use_fidelity/response.schema.json",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "tool_use_fidelity Response",
  "type": "object",
  "required": ["feature_id", "model_id", "metrics", "by_probe"],
  "properties": {
    "feature_id": { "type": "string", "const": "tool_use_fidelity" },
    "model_id": { "type": "string" },
    "metrics": {
      "type": "object",
      "required": ["reproducibility_rate", "latency_p95_ms"],
      "properties": {
        "reproducibility_rate": { "type": "number", "minimum": 0, "maximum": 1 },
        "robustness_delta": { "type": "number" },
        "latency_p95_ms": { "type": "number", "minimum": 0 }
      },
      "additionalProperties": true
    },
    "by_probe": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["id", "status", "elapsed_ms"],
        "properties": {
          "id": { "type": "string" },
          "status": { "type": "string", "enum": ["ok", "error"] },
          "elapsed_ms": { "type": "number", "minimum": 0 },
          "error_class": { "type": "string" }
        },
        "additionalProperties": false
      }
    }
  },
  "additionalProperties": false
}
```

---

### **Update**

### **backend/app/routes/tool_use.py**

```
from fastapi import APIRouter, HTTPException, Header
from pydantic import BaseModel
from backend.app.eval.runner import run_batch
from backend.app.schemas.request_schema import REQUEST_SCHEMA
from backend.app.schemas.response_schema import RESPONSE_SCHEMA
from backend.app.obs.metrics import EVAL_REQS, EVAL_DUR
from jsonschema import validate as js_validate, ValidationError
import time

router = APIRouter()

class EvaluateRequest(BaseModel):
    feature_id: str
    probes: list
    config: dict

@router.post("/tool-use/evaluate")
def evaluate(req: EvaluateRequest, x_api_key: str = Header(default="")):
    if not x_api_key:
        raise HTTPException(status_code=401, detail="Missing API key")

    t0 = time.perf_counter()
    try:
        js_validate(req.model_dump(), REQUEST_SCHEMA)
    except ValidationError as e:
        raise HTTPException(status_code=400, detail={"error": "schema", "msg": str(e)}) from e

    resp = run_batch(req.model_dump())

    try:
        js_validate(resp, RESPONSE_SCHEMA)
    except ValidationError as e:
        raise HTTPException(status_code=500, detail={"error": "contract", "msg": str(e)}) from e

    dur_ms = (time.perf_counter() - t0) * 1000.0
    EVAL_REQS.labels(feature=req.feature_id, model=resp.get("model_id", "unknown")).inc()
    EVAL_DUR.labels(feature=req.feature_id).observe(dur_ms)

    return resp
```

---

### **New test:**

### **backend/tests/test_api_contract.py**

```
from fastapi.testclient import TestClient
from backend.app.main import app

client = TestClient(app)

def _req():
    return {
        "feature_id":"tool_use_fidelity",
        "probes":[
            {
                "id":"p001",
                "prompt":"Call weather.get_forecast for city='Paris' and date='2025-10-05'.",
                "expected":{"tool":"weather.get_forecast","args":{"city":"Paris","date":"2025-10-05"}}
            }
        ],
        "config":{"model_id":"qwen-2.5-instruct","provider":"openrouter","seeds":[1,2,3,4,5],"decoding":{"temperature":0,"top_p":1}}
    }

def test_evaluate_200_and_schema_valid():
    r = client.post("/api/tool-use/evaluate", json=_req(), headers={"X-API-Key":"test"})
    assert r.status_code == 200
    body = r.json()
    assert body["feature_id"] == "tool_use_fidelity"
    assert "reproducibility_rate" in body["metrics"]

def test_evaluate_400_on_bad_schema():
    bad = _req()
    del bad["probes"][0]["expected"]
    r = client.post("/api/tool-use/evaluate", json=bad, headers={"X-API-Key":"test"})
    assert r.status_code == 400

def test_metrics_endpoint_exists():
    m = client.get("/metrics")
    assert m.status_code == 200
```

---

### **New test:**

### **backend/tests/test_determinism_golden.py**

```
from fastapi.testclient import TestClient
from backend.app.main import app

client = TestClient(app)

REQ = {
    "feature_id":"tool_use_fidelity",
    "probes":[
        {"id":"p001","prompt":"Weather Paris 2025-10-05","expected":{"tool":"weather.get_forecast","args":{"city":"Paris","date":"2025-10-05"}}},
        {"id":"p002","prompt":"Create 'Team Sync' 2025-10-06 14:00","expected":{"tool":"calendar.create_event","args":{"title":"Team Sync","date":"2025-10-06","time":"14:00"}}}
    ],
    "config":{"model_id":"qwen-2.5-instruct","provider":"openrouter","seeds":[1,2,3,4,5],"decoding":{"temperature":0,"top_p":1}}
}

def test_determinism_same_hashes_two_runs():
    headers = {"X-API-Key":"test"}
    r1 = client.post("/api/tool-use/evaluate", json=REQ, headers=headers).json()
    r2 = client.post("/api/tool-use/evaluate", json=REQ, headers=headers).json()
    assert r1["hashes"] == r2["hashes"]
```

---

### **.gitignore**

###  **(root)**

```
.venv/
__pycache__/
*.pyc
*.pyo
.pytest_cache/
.coverage
htmlcov/
.env
.DS_Store
```

---

### **Optional helper:**

### **tools/golden/compute_hashes.py**

```
#!/usr/bin/env python3
import json, sys, hashlib

def h(s): return hashlib.sha256(s.encode("utf-8")).hexdigest()

def main():
    data = json.load(sys.stdin)
    hashes = data.get("hashes", {})
    for k,v in hashes.items():
        print(f"{k}\t{v}")

if __name__ == "__main__":
    main()
```

---

### **CI tweak (no server needed; tests hit app in-process)**

No change required to ci-m3.yml. Pytests now include API, schema, and determinism checks.

---

### **Final M3 self-check**

```
# From repo root
make fmt
make type
make test     # runs unit + contract + determinism tests
```

M3 is now complete once CI passes and the PR is approved and merged.
